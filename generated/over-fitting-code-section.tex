\subsection{The siren call of over-fitting code}\label{over-fit}
%Science is a methodical process that constructs principles and theories that describe the world, and competitive peer reviewed publication is the reliable record of that. We do the science, and we are in the world, so there are challenging meta-theoretical issues as well as practical experimental and resourcing issues. Karl Popper is one of many philosophers of science to develop principles and theories about science \cite{popper-three-worlds,popper-conjectures-refutations}. 

%Popper was concerned with the conventional sciences, where the world, Nature, is assumed fixed and in principle knowable. Writing just six years after Popper, Herbert Simon introduced the ``sciences of the artificial'' \cite{sciences-artificial}, which instead focus on understanding artificial worlds, most typically computer programs. Programs effectively create worlds, and the sciences of the artificial seek to understand and improve those worlds. Crucially, there is no philosophical or other reason why artificial worlds should be based on principles, let alone be simple or elegant. Conventional science's \ae sthetics need not apply.

%Science to date has been inevitably driven by human \ae sthetics: that is, simple, elegant, understandable, beautiful, brief, exciting, intelligible \ldots\ all are human properties that are applied to doing, funding, defining, and selecting ``good'' science. 

\noindent
Poor code undermines scientific quality: poor code can generate plausible and possibly misleading results from \emph{any\/} data, including even fictional science. A temptation is that %artificial world \cite{sciences-artificial}, built on
adjusting the code's results gets more attention than the theoretical basis of the code's overall faithfulness to the experimental phenomena. 
% wolframscript (if being used) unnecessarily prints the next 2 lines (generated by CellPrint), but they are needed!
\def\imageWidthCalculation{\multiply \imageWidth by 21 \divide \imageWidth by 28}
\def\figureStarDetails{*}

% wolframscript (if being used) unnecessarily prints the next 2 lines (generated by CellPrint), but they are needed because the text also ends up in notebook cells!
\def\imageWidthCalculation{\multiply \imageWidth by 21 \divide \imageWidth by 28}
\def\figureStarDetails{*}

\begin{figure\figureStarDetails}[t]
{ \newdimen \imageWidth 
  \imageWidth=\textwidth
  \imageWidthCalculation
  \begin{center}\includegraphics[width=\imageWidth]{
generated/mathematicaplot.jpg
}\end{center}}
\caption{Much computational science is concerned with finding plausible multi-dimensional models that fit models to data with the aim of extrapolating or predicting new results from them. Shown here is notional sample of experimental 2D data (the dots), a linear least squares regression, and an exact polynomial model. The over-fitted polynomial model fits the sample \emph{exactly\/}, but since the experimental data is presumably subject to random error (indicated by the confidence interval, itself estimated) the linear model would generally be considered a better description of the experimental data.}
\label{fig-overfit}
\end{figure\figureStarDetails}

In modeling terms, superficially successful computer programs tend to \emph{over-fit\/} phenomena \cite{over-fit}. Instead of using code to test the models, we tinker with the code adapting it to be closer and closer to our prejudices. The code then apparently confirms our science, since we fitted it to our understanding.  

In general, an \emph{over-fitted model\/} fits a set of data closely, but contains more parameters than can be justified by the science. An over-fitted model fails to reliably predict results beyond the scope of the data it has been fitted to. Over-fitting is a well-known problem, but the point is that when code is used, over-fitting is done unconsciously by programmers adjusting the code --- its parameters, its structure, its embedded data, the calculations it performs --- that specifies the model. ``A model over-fits if it is  more complex than another model that fits equally well'' \cite{over-fit}, is a criterion that describes almost every program! Programmers without the discipline and experience to manage the unlimited adaptability of code debug, alter, and extend their code to make it do what they think it should do. This becomes a vicious circle as the idea of what the science is becomes driven by the code. Rather than debugging code by improving its fit to the actual science, it gets debugged and extended to fit the expectations.  

The problems of over-fitting data may be visualized using a Real$\rightarrow$Real function of one variable (figure \ref{fig-overfit}). The code that generates an over-fitted curve seems to work very well: in the example shown, the over-fitted curve fits the sampled data exactly; indeed, the code used here will fit any new data exactly as well. But the code has a negligible ability to predict new data or to describe theories of the data, which is the point of modeling. The fact that over-fitted code seems to work well is deceptive.

Looking specifically at the data plotted in figure \ref{fig-overfit} if, for the sake of argument, we assume the error in the data is normally distributed then the values the over-fitted code generates outside the range of the sample are improbable. For example, the basic linear model predicts
$\hat{y}(0)=0.5$, 
versus the extreme value predicted by the over-fitting code, 
$\hat{y}(0)=-41.8$, 